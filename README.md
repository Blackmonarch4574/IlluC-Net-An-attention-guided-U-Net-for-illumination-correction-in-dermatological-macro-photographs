# IlluC-Net: An Attention-Guided U-Net for Illumination Correction in Dermatological Macro-Photographs

## Overview

Uneven illumination in dermatological macro-photographs can significantly degrade the performance of automated diagnostic systems, including lesion segmentation, classification, and texture analysis. Traditional enhancement methods struggle to handle challenging lighting artifacts like shadows, hotspots, and local under-exposure, often over-correcting images or destroying clinically meaningful information.

**IlluC-Net** is a novel **attention-guided U-Net architecture** designed specifically for illumination correction in dermatological macro-images. By integrating spatial attention into an enhanced U-Net backbone, IlluC-Net effectively normalizes illumination while preserving fine-grained skin structures essential for clinical interpretation.

---

## Key Features

- **Attention-Enhanced U-Net Architecture**  
  Integrates spatial attention modules into skip connections for improved focus on under- or over-illuminated regions.

- **End-to-End Illumination Correction**  
  Produces uniformly illuminated images without sacrificing dermatological texture details.

- **Robust Against Illumination Artifacts**  
  Handles shadows, glare, brightness inconsistencies, lens reflections, and micro-illumination variations.

- **Pretrained Model Included**  
  Allows immediate testing and fine-tuning for custom datasets.

- **Modular & Clean Codebase**  
  Designed for research use, extension, and integration into medical imaging pipelines.

---

## Model Architecture

IlluC-Net combines:

- A **U-Net encoder-decoder backbone** for dense image-to-image correction.
- **Spatial Attention Modules** to emphasize illumination-affected areas.
- Optional **residual refinement blocks** for deeper feature propagation.

This hybrid design ensures a balance between local detail preservation and global illumination normalization.

---

# Performance Evaluation

IlluC-Net was evaluated on 100 curated dermatological test images with diverse illumination artifacts. It outperformed several state-of-the-art models in both pixel-level and perceptual metrics.

---

## Table 1 â€” Quantitative Comparison on 100-Test Images  
*(mean Â± standard deviation; best results in bold)*

| Method | PSNR â†‘ | SSIM â†‘ | MSE â†“ | Entropy â†‘ | BRISQUE â†“ | PIQE â†“ | NIQE â†“ |
|--------|--------|--------|--------|------------|-----------|-----------|-----------|
| U-Net [25] (2020) | 31.55 Â± 4.95 | 0.96 Â± 0.01 | 0.027 Â± 0.001 | 6.85 Â± 0.14 | 35.48 Â± 2.46 | 49.06 Â± 7.84 | 0.31 Â± 0.03 |
| StillGAN [26] (2021) | 28.92 Â± 6.42 | 0.79 Â± 0.12 | 0.028 Â± 0.002 | 6.57 Â± 0.30 | 29.96 Â± 5.58 | 49.04 Â± 3.19 | 0.31 Â± 0.04 |
| EnlightenGAN [27] (2021) | 30.22 Â± 2.02 | 0.82 Â± 0.06 | 0.033 Â± 0.005 | 6.78 Â± 0.23 | 47.28 Â± 0.88 | 48.07 Â± 0.53 | 0.32 Â± 0.02 |
| TransUNet [29] (2022) | 33.23 Â± 1.01 | 0.88 Â± 0.04 | 0.031 Â± 0.001 | 6.76 Â± 0.25 | 48.32 Â± 6.95 | 46.91 Â± 6.25 | 0.30 Â± 0.02 |
| CSwin-P [31] (2023) | 32.83 Â± 0.41 | 0.82 Â± 0.03 | 0.029 Â± 0.001 | 6.47 Â± 0.23 | 61.94 Â± 1.09 | 54.59 Â± 3.27 | 0.25 Â± 0.02 |
| IECET [21] (2024) | 26.22 Â± 3.31 | 0.77 Â± 0.02 | 0.041 Â± 0.002 | 6.45 Â± 0.08 | 35.25 Â± 6.76 | 62.53 Â± 6.34 | 0.52 Â± 0.05 |
| Suchitra et al. [34] (2025) | 23.63 Â± 0.39 | 0.76 Â± 0.01 | 0.044 Â± 0.004 | 6.75 Â± 0.07 | 40.17 Â± 8.49 | 65.78 Â± 4.17 | 0.51 Â± 0.03 |
| **IlluC-Net (Proposed)** | **35.99 Â± 3.15** | **0.98 Â± 0.01** | **0.026 Â± 0.002** | 6.70 Â± 0.25 | **27.09 Â± 10.43** | **44.39 Â± 9.77** | **0.27 Â± 0.02** |

---

## Table 2 â€” Model Complexity & Inference Time

| Method | Parameters (M) | Inference Time per Image (ms) |
|--------|----------------|-------------------------------|
| U-Net [25] (2020) | 30.8 | 90 |
| StillGAN [26] (2021) | 42.4 | 150 |
| EnlightenGAN [27] (2021) | 36.2 | 110 |
| TransUNet [29] (2022) | 41.4 | 100 |
| CSwin-P [31] (2023) | 72.8 | 92 |
| IECET [21] (2024) | 10.5 | 170 |
| Suchitra et al. [34] (2025) | â€“ | 110 |
| **IlluC-Net (Proposed)** | **31.3** | **90** |

---

## Installation

```bash
git clone https://github.com/Blackmonarch4574/IlluC-Net-An-attention-guided-U-Net-for-illumination-correction-in-dermatological-macro-photographs.git
cd IlluC-Net

```
## Data Set 

The IlluC-Net model was developed and evaluated using dermatological macro-photographs collected from publicly available repositories and custom clinical image sets. These datasets contain a wide variety of illumination artifacts such as shadows, glare, under-exposed regions, specular highlights, and uneven lighting patternsâ€”making them ideal for benchmarking illumination correction algorithms.

ðŸ”— https://data.mendeley.com/datasets/jdpnmyk696/1


## Citation

If you use this work, please cite:

```bibtex
@article{ADITHYA2025100439,
title = {IlluC-Net: An attention-guided U-Net for illumination correction in dermatological macro-photographs},
journal = {Franklin Open},
volume = {13},
pages = {100439},
year = {2025},
issn = {2773-1863},
doi = {https://doi.org/10.1016/j.fraope.2025.100439},
url = {https://www.sciencedirect.com/science/article/pii/S2773186325002245},
author = {Likith Adithya and Gowtham Sai Chandra and Sri Naga Harshavardhan and Ruthvik Guptha and Anoop B.N. and Vipin Venugopal},
keywords = {Attention mechanism, Deep learning, Illumination correction, Macro-photographs, Medical imaging, U-Net}
}
```
